---
layout: post
title:  "Learner 002"
date:   2020-11-19
tags: learning ubuntu
---

## 1. Ubuntu tracker issue - followup

To follow up the ubuntu track issue, I left a comment in one thoe [post][reference1] to ask if the changes has been released and they came back to me really quickly said it has been released and suggesed me several ways to debug.

> Some things to check:

* which process is taking up CPU? tracker is just the CLI tool
* what directories are indexed -- is there a huge amount of content in any of them? (If you haven't changed the search config, https://gnome.pages.gitlab.gnome.org/tracker/faq/#what-files-will-tracker-index )
* what does tracker status say?
* are there any errors reported in the system log? (you can use journalctl or GNOME Logs to check)

I did the tracker status several times, but get two different kind of results:
1. > Currently indexed: 0 files, 0 folders
Remaining space on database partition: 114.0Â GB (61.37%)
(tracker status:8010): Tracker-CRITICAL **: 09:58:30.684: Could not get miner progress for 'org.freedesktop.Tracker1.Miner.Files': Timeout was reached
All data miners are idle, indexing complete

2. > Currently indexed: x files, x folders

When second scenario happens, although tracker miner was still taking 100%cpu at the time, it will automatically stop the process and release the resources when the indexing is finished

However, when i went through the system logs there were no tracker errors in it. 

So the investigation is still going. But for now I added a .trackignore file to my folder which has a lot of images in it.


[reference1]:https://gitlab.gnome.org/GNOME/tracker-miners/-/issues/117


## 2. Deep learning training - reproducibitily 

Recently I have been toying a lot with a kaggle playground project which does image clasification, mostly using the fastai library

FasiAI is a high level library based on pytorch. 

One of the big issue I had is reproducibitily. One reason is there are a lot of parameters thrown into the process.  The second one is altough high level library is really handy, but at the same time you don't really know what is actually going on under the hood and a lot of default parameters that are already set for you. 

However, we can think from the top, 
1. what might cause the result not reproducible? randomness! 
2. what can create randomness? Ramdonly shuffled and transformed dataloads (aka input) and randomly instialized weights!

So I think when you can't reproduce a result, the first fews things to check if dataloads and weights have changed.

Of course, there are so many other things to think aobut, such as one of Pytorch's [note][ref2]describled here. But we have off for a good start, not to get rid of the randomness (which is a key to model's generilization), but to control and understand the randomness.

And based on this, I think fastai's lr_find function has some issue in it, which I have put it github, hopefully they will come back to me soon.

[ref2]https://pytorch.org/docs/stable/notes/randomness.html

## 3. Deep learning training - parameters

So far the experiment I have done showed the fastai fine_tune function keeps extremely well.

